{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "I3X1dRawK2NN",
        "outputId": "6037bcfc-68b7-4138-ba76-f37cf8672faa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4mbRqp6K_GX"
      },
      "source": [
        "#1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pabpV8nnK-Aj",
        "outputId": "0f5977b0-0833-4593-da3a-8ed1b45df5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 168864176.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 35071133.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 60031695.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5474289.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "img_size = 32\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((img_size, img_size)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "images = torchvision.datasets.MNIST(root='./mnist_data', train=True,\n",
        "                                    download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cbC8fi5M-To"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "dataloader = torch.utils.data.DataLoader(images, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAkcst9KNIyo",
        "outputId": "6fbcd46a-62eb-4d26-9b65-12b079e976b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           ...,\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
              " \n",
              " \n",
              "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           ...,\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
              " \n",
              " \n",
              "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           ...,\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           ...,\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
              " \n",
              " \n",
              "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           ...,\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
              " \n",
              " \n",
              "         [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           ...,\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "           [-1., -1., -1.,  ..., -1., -1., -1.]]]]),\n",
              " tensor([9, 3, 9, 1, 9, 2, 6, 7, 3, 6, 3, 4, 1, 1, 4, 1, 1, 3, 2, 3, 4, 5, 3, 6,\n",
              "         9, 5, 1, 0, 1, 0, 0, 5, 0, 7, 3, 1, 3, 8, 9, 1, 1, 4, 1, 3, 7, 4, 5, 0,\n",
              "         5, 3, 2, 3, 8, 9, 2, 9, 7, 1, 2, 0, 1, 6, 7, 4])]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unKEGfmHNQYX"
      },
      "source": [
        "# 2. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szLxxBrINLR0"
      },
      "outputs": [],
      "source": [
        "channels = 1\n",
        "img_shape = (channels, img_size, img_size)\n",
        "latent_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2_2eRMDNZtB"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            #nn.BatchNorm1d(128),\n",
        "            #nn.LeakyReLU(0.2, inplace=True),\n",
        "            #nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        img = self.model(x)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SM-dKEuPplP"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = img.view(img.size(0), -1)\n",
        "        x = self.model(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX3dljE_QXc5"
      },
      "outputs": [],
      "source": [
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onHDiTyAQ1FJ"
      },
      "source": [
        "# 3. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gki9gX9EQqoQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "save_interval = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7VIo7lvQ9AB",
        "outputId": "6f088b17-a9ac-4090-825f-45b0159e5ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Train G Loss: 9.2706, Train D Loss: 0.0501\n",
            "Epoch [2/200], Train G Loss: 7.1514, Train D Loss: 0.0589\n",
            "Epoch [3/200], Train G Loss: 6.8083, Train D Loss: 0.0360\n",
            "Epoch [4/200], Train G Loss: 6.9454, Train D Loss: 0.0539\n",
            "Epoch [5/200], Train G Loss: 6.8472, Train D Loss: 0.0626\n",
            "Epoch [6/200], Train G Loss: 6.7131, Train D Loss: 0.0722\n",
            "Epoch [7/200], Train G Loss: 6.0395, Train D Loss: 0.1042\n",
            "Epoch [8/200], Train G Loss: 5.7264, Train D Loss: 0.1021\n",
            "Epoch [9/200], Train G Loss: 5.8611, Train D Loss: 0.0998\n",
            "Epoch [10/200], Train G Loss: 5.7438, Train D Loss: 0.1088\n",
            "Epoch [11/200], Train G Loss: 5.3954, Train D Loss: 0.1159\n",
            "Epoch [12/200], Train G Loss: 5.2642, Train D Loss: 0.1328\n",
            "Epoch [13/200], Train G Loss: 4.7555, Train D Loss: 0.1529\n",
            "Epoch [14/200], Train G Loss: 4.6534, Train D Loss: 0.1568\n",
            "Epoch [15/200], Train G Loss: 4.2824, Train D Loss: 0.1653\n",
            "Epoch [16/200], Train G Loss: 4.3738, Train D Loss: 0.1807\n",
            "Epoch [17/200], Train G Loss: 4.0368, Train D Loss: 0.1953\n",
            "Epoch [18/200], Train G Loss: 3.9708, Train D Loss: 0.2045\n",
            "Epoch [19/200], Train G Loss: 3.8769, Train D Loss: 0.2142\n",
            "Epoch [20/200], Train G Loss: 3.6342, Train D Loss: 0.2385\n",
            "Epoch [21/200], Train G Loss: 3.4935, Train D Loss: 0.2435\n",
            "Epoch [22/200], Train G Loss: 3.2892, Train D Loss: 0.2631\n",
            "Epoch [23/200], Train G Loss: 3.2614, Train D Loss: 0.2595\n",
            "Epoch [24/200], Train G Loss: 3.1299, Train D Loss: 0.2717\n",
            "Epoch [25/200], Train G Loss: 3.2217, Train D Loss: 0.2770\n",
            "Epoch [26/200], Train G Loss: 3.1573, Train D Loss: 0.2857\n",
            "Epoch [27/200], Train G Loss: 3.1761, Train D Loss: 0.2684\n",
            "Epoch [28/200], Train G Loss: 2.9091, Train D Loss: 0.2961\n",
            "Epoch [29/200], Train G Loss: 2.9179, Train D Loss: 0.2939\n",
            "Epoch [30/200], Train G Loss: 2.8304, Train D Loss: 0.3057\n",
            "Epoch [31/200], Train G Loss: 2.7441, Train D Loss: 0.3115\n",
            "Epoch [32/200], Train G Loss: 2.6255, Train D Loss: 0.3191\n",
            "Epoch [33/200], Train G Loss: 2.6681, Train D Loss: 0.3107\n",
            "Epoch [34/200], Train G Loss: 2.6528, Train D Loss: 0.3107\n",
            "Epoch [35/200], Train G Loss: 2.5402, Train D Loss: 0.3295\n",
            "Epoch [36/200], Train G Loss: 2.5245, Train D Loss: 0.3291\n",
            "Epoch [37/200], Train G Loss: 2.3759, Train D Loss: 0.3374\n",
            "Epoch [38/200], Train G Loss: 2.3822, Train D Loss: 0.3342\n",
            "Epoch [39/200], Train G Loss: 2.4135, Train D Loss: 0.3405\n",
            "Epoch [40/200], Train G Loss: 2.2533, Train D Loss: 0.3618\n",
            "Epoch [41/200], Train G Loss: 2.2663, Train D Loss: 0.3467\n",
            "Epoch [42/200], Train G Loss: 2.3061, Train D Loss: 0.3548\n",
            "Epoch [43/200], Train G Loss: 2.1827, Train D Loss: 0.3635\n",
            "Epoch [44/200], Train G Loss: 2.2292, Train D Loss: 0.3535\n",
            "Epoch [45/200], Train G Loss: 2.1867, Train D Loss: 0.3676\n",
            "Epoch [46/200], Train G Loss: 2.1824, Train D Loss: 0.3692\n",
            "Epoch [47/200], Train G Loss: 2.1637, Train D Loss: 0.3692\n",
            "Epoch [48/200], Train G Loss: 2.1671, Train D Loss: 0.3750\n",
            "Epoch [49/200], Train G Loss: 2.1300, Train D Loss: 0.3811\n",
            "Epoch [50/200], Train G Loss: 2.0712, Train D Loss: 0.3871\n",
            "Epoch [51/200], Train G Loss: 2.0407, Train D Loss: 0.3958\n",
            "Epoch [52/200], Train G Loss: 2.0608, Train D Loss: 0.3921\n",
            "Epoch [53/200], Train G Loss: 2.0782, Train D Loss: 0.3885\n",
            "Epoch [54/200], Train G Loss: 2.0719, Train D Loss: 0.3916\n",
            "Epoch [55/200], Train G Loss: 2.0625, Train D Loss: 0.3928\n",
            "Epoch [56/200], Train G Loss: 1.9945, Train D Loss: 0.3984\n",
            "Epoch [57/200], Train G Loss: 1.9727, Train D Loss: 0.3917\n",
            "Epoch [58/200], Train G Loss: 1.9973, Train D Loss: 0.3978\n",
            "Epoch [59/200], Train G Loss: 2.0680, Train D Loss: 0.3862\n",
            "Epoch [60/200], Train G Loss: 1.9325, Train D Loss: 0.4127\n",
            "Epoch [61/200], Train G Loss: 1.9465, Train D Loss: 0.4100\n",
            "Epoch [62/200], Train G Loss: 2.0007, Train D Loss: 0.3971\n",
            "Epoch [63/200], Train G Loss: 1.9682, Train D Loss: 0.4070\n",
            "Epoch [64/200], Train G Loss: 1.9394, Train D Loss: 0.4068\n",
            "Epoch [65/200], Train G Loss: 1.9891, Train D Loss: 0.3846\n",
            "Epoch [66/200], Train G Loss: 1.9477, Train D Loss: 0.4130\n",
            "Epoch [67/200], Train G Loss: 1.9881, Train D Loss: 0.3929\n",
            "Epoch [68/200], Train G Loss: 1.9331, Train D Loss: 0.4097\n",
            "Epoch [69/200], Train G Loss: 1.9409, Train D Loss: 0.3936\n",
            "Epoch [70/200], Train G Loss: 1.9686, Train D Loss: 0.3944\n",
            "Epoch [71/200], Train G Loss: 1.9654, Train D Loss: 0.4059\n",
            "Epoch [72/200], Train G Loss: 1.9586, Train D Loss: 0.3956\n",
            "Epoch [73/200], Train G Loss: 1.9559, Train D Loss: 0.4012\n",
            "Epoch [74/200], Train G Loss: 1.9627, Train D Loss: 0.4059\n",
            "Epoch [75/200], Train G Loss: 1.9765, Train D Loss: 0.3889\n",
            "Epoch [76/200], Train G Loss: 2.0097, Train D Loss: 0.4007\n",
            "Epoch [77/200], Train G Loss: 2.0091, Train D Loss: 0.4008\n",
            "Epoch [78/200], Train G Loss: 2.0039, Train D Loss: 0.4015\n",
            "Epoch [79/200], Train G Loss: 1.9862, Train D Loss: 0.3908\n",
            "Epoch [80/200], Train G Loss: 1.9860, Train D Loss: 0.4012\n",
            "Epoch [81/200], Train G Loss: 1.9912, Train D Loss: 0.4078\n",
            "Epoch [82/200], Train G Loss: 1.9722, Train D Loss: 0.3998\n",
            "Epoch [83/200], Train G Loss: 1.9503, Train D Loss: 0.3978\n",
            "Epoch [84/200], Train G Loss: 2.0009, Train D Loss: 0.4155\n",
            "Epoch [85/200], Train G Loss: 1.9870, Train D Loss: 0.4053\n",
            "Epoch [86/200], Train G Loss: 1.9543, Train D Loss: 0.4116\n",
            "Epoch [87/200], Train G Loss: 2.0019, Train D Loss: 0.3897\n",
            "Epoch [88/200], Train G Loss: 1.9942, Train D Loss: 0.3926\n",
            "Epoch [89/200], Train G Loss: 2.0082, Train D Loss: 0.3951\n",
            "Epoch [90/200], Train G Loss: 2.0336, Train D Loss: 0.3934\n",
            "Epoch [91/200], Train G Loss: 2.0103, Train D Loss: 0.3945\n",
            "Epoch [92/200], Train G Loss: 2.0232, Train D Loss: 0.3801\n",
            "Epoch [93/200], Train G Loss: 2.0532, Train D Loss: 0.3944\n",
            "Epoch [94/200], Train G Loss: 2.0463, Train D Loss: 0.3905\n",
            "Epoch [95/200], Train G Loss: 2.0954, Train D Loss: 0.3694\n",
            "Epoch [96/200], Train G Loss: 2.0971, Train D Loss: 0.3941\n",
            "Epoch [97/200], Train G Loss: 2.1157, Train D Loss: 0.4057\n",
            "Epoch [98/200], Train G Loss: 2.0659, Train D Loss: 0.3953\n",
            "Epoch [99/200], Train G Loss: 2.0858, Train D Loss: 0.3786\n",
            "Epoch [100/200], Train G Loss: 2.0633, Train D Loss: 0.3795\n",
            "Epoch [101/200], Train G Loss: 2.0871, Train D Loss: 0.3730\n",
            "Epoch [102/200], Train G Loss: 2.1294, Train D Loss: 0.3781\n",
            "Epoch [103/200], Train G Loss: 2.1426, Train D Loss: 0.3683\n",
            "Epoch [104/200], Train G Loss: 2.1550, Train D Loss: 0.3668\n",
            "Epoch [105/200], Train G Loss: 2.1822, Train D Loss: 0.3560\n",
            "Epoch [106/200], Train G Loss: 2.2040, Train D Loss: 0.3621\n",
            "Epoch [107/200], Train G Loss: 2.2043, Train D Loss: 0.3573\n",
            "Epoch [108/200], Train G Loss: 2.2475, Train D Loss: 0.3505\n",
            "Epoch [109/200], Train G Loss: 2.2728, Train D Loss: 0.3518\n",
            "Epoch [110/200], Train G Loss: 2.2991, Train D Loss: 0.3440\n",
            "Epoch [111/200], Train G Loss: 2.2980, Train D Loss: 0.3518\n",
            "Epoch [112/200], Train G Loss: 2.3070, Train D Loss: 0.3451\n",
            "Epoch [113/200], Train G Loss: 2.3049, Train D Loss: 0.3391\n",
            "Epoch [114/200], Train G Loss: 2.3326, Train D Loss: 0.3343\n",
            "Epoch [115/200], Train G Loss: 2.3647, Train D Loss: 0.3437\n",
            "Epoch [116/200], Train G Loss: 2.4025, Train D Loss: 0.3377\n",
            "Epoch [117/200], Train G Loss: 2.3908, Train D Loss: 0.3344\n",
            "Epoch [118/200], Train G Loss: 2.3976, Train D Loss: 0.3386\n",
            "Epoch [119/200], Train G Loss: 2.4240, Train D Loss: 0.3166\n",
            "Epoch [120/200], Train G Loss: 2.4669, Train D Loss: 0.3227\n",
            "Epoch [121/200], Train G Loss: 2.5056, Train D Loss: 0.3193\n",
            "Epoch [122/200], Train G Loss: 2.5108, Train D Loss: 0.3189\n",
            "Epoch [123/200], Train G Loss: 2.5352, Train D Loss: 0.3148\n",
            "Epoch [124/200], Train G Loss: 2.5825, Train D Loss: 0.3142\n",
            "Epoch [125/200], Train G Loss: 2.5426, Train D Loss: 0.3087\n",
            "Epoch [126/200], Train G Loss: 2.6035, Train D Loss: 0.3018\n",
            "Epoch [127/200], Train G Loss: 2.6038, Train D Loss: 0.2999\n",
            "Epoch [128/200], Train G Loss: 2.6339, Train D Loss: 0.3218\n",
            "Epoch [129/200], Train G Loss: 2.6133, Train D Loss: 0.3016\n",
            "Epoch [130/200], Train G Loss: 2.6652, Train D Loss: 0.3095\n",
            "Epoch [131/200], Train G Loss: 2.6307, Train D Loss: 0.3095\n",
            "Epoch [132/200], Train G Loss: 2.6215, Train D Loss: 0.2946\n",
            "Epoch [133/200], Train G Loss: 2.6928, Train D Loss: 0.3071\n",
            "Epoch [134/200], Train G Loss: 2.6810, Train D Loss: 0.3002\n",
            "Epoch [135/200], Train G Loss: 2.6950, Train D Loss: 0.3046\n",
            "Epoch [136/200], Train G Loss: 2.6930, Train D Loss: 0.3090\n",
            "Epoch [137/200], Train G Loss: 2.6990, Train D Loss: 0.2899\n",
            "Epoch [138/200], Train G Loss: 2.7228, Train D Loss: 0.2832\n",
            "Epoch [139/200], Train G Loss: 2.7836, Train D Loss: 0.2880\n",
            "Epoch [140/200], Train G Loss: 2.7979, Train D Loss: 0.2772\n",
            "Epoch [141/200], Train G Loss: 2.8214, Train D Loss: 0.2726\n",
            "Epoch [142/200], Train G Loss: 2.8521, Train D Loss: 0.2807\n",
            "Epoch [143/200], Train G Loss: 2.8809, Train D Loss: 0.2762\n",
            "Epoch [144/200], Train G Loss: 2.8627, Train D Loss: 0.2731\n",
            "Epoch [145/200], Train G Loss: 2.8950, Train D Loss: 0.2816\n",
            "Epoch [146/200], Train G Loss: 2.9168, Train D Loss: 0.2807\n",
            "Epoch [147/200], Train G Loss: 2.8796, Train D Loss: 0.2746\n",
            "Epoch [148/200], Train G Loss: 2.9435, Train D Loss: 0.2723\n",
            "Epoch [149/200], Train G Loss: 2.9818, Train D Loss: 0.2733\n",
            "Epoch [150/200], Train G Loss: 2.9181, Train D Loss: 0.2892\n",
            "Epoch [151/200], Train G Loss: 2.9786, Train D Loss: 0.2690\n",
            "Epoch [152/200], Train G Loss: 2.9525, Train D Loss: 0.2659\n",
            "Epoch [153/200], Train G Loss: 2.9430, Train D Loss: 0.2551\n",
            "Epoch [154/200], Train G Loss: 3.0033, Train D Loss: 0.2546\n",
            "Epoch [155/200], Train G Loss: 3.0535, Train D Loss: 0.2545\n",
            "Epoch [156/200], Train G Loss: 3.0126, Train D Loss: 0.2649\n",
            "Epoch [157/200], Train G Loss: 2.9997, Train D Loss: 0.2546\n",
            "Epoch [158/200], Train G Loss: 3.0165, Train D Loss: 0.2422\n",
            "Epoch [159/200], Train G Loss: 2.9812, Train D Loss: 0.2655\n",
            "Epoch [160/200], Train G Loss: 3.0679, Train D Loss: 0.2718\n",
            "Epoch [161/200], Train G Loss: 3.0727, Train D Loss: 0.2633\n",
            "Epoch [162/200], Train G Loss: 3.0516, Train D Loss: 0.2557\n",
            "Epoch [163/200], Train G Loss: 3.1118, Train D Loss: 0.2436\n",
            "Epoch [164/200], Train G Loss: 3.0958, Train D Loss: 0.2471\n",
            "Epoch [165/200], Train G Loss: 3.1064, Train D Loss: 0.2509\n",
            "Epoch [166/200], Train G Loss: 3.1260, Train D Loss: 0.2748\n",
            "Epoch [167/200], Train G Loss: 3.0499, Train D Loss: 0.2449\n",
            "Epoch [168/200], Train G Loss: 3.1154, Train D Loss: 0.2547\n",
            "Epoch [169/200], Train G Loss: 3.1084, Train D Loss: 0.2491\n",
            "Epoch [170/200], Train G Loss: 3.0798, Train D Loss: 0.2541\n",
            "Epoch [171/200], Train G Loss: 3.0818, Train D Loss: 0.2381\n",
            "Epoch [172/200], Train G Loss: 3.1499, Train D Loss: 0.2482\n",
            "Epoch [173/200], Train G Loss: 3.1262, Train D Loss: 0.2528\n",
            "Epoch [174/200], Train G Loss: 3.1266, Train D Loss: 0.2369\n",
            "Epoch [175/200], Train G Loss: 3.1870, Train D Loss: 0.2611\n",
            "Epoch [176/200], Train G Loss: 3.1348, Train D Loss: 0.2681\n",
            "Epoch [177/200], Train G Loss: 3.1316, Train D Loss: 0.2485\n",
            "Epoch [178/200], Train G Loss: 3.1723, Train D Loss: 0.2434\n",
            "Epoch [179/200], Train G Loss: 3.1676, Train D Loss: 0.2540\n",
            "Epoch [180/200], Train G Loss: 3.1303, Train D Loss: 0.2398\n",
            "Epoch [181/200], Train G Loss: 3.1947, Train D Loss: 0.2587\n",
            "Epoch [182/200], Train G Loss: 3.2130, Train D Loss: 0.2517\n",
            "Epoch [183/200], Train G Loss: 3.1877, Train D Loss: 0.2451\n",
            "Epoch [184/200], Train G Loss: 3.1630, Train D Loss: 0.2455\n",
            "Epoch [185/200], Train G Loss: 3.1962, Train D Loss: 0.2257\n",
            "Epoch [186/200], Train G Loss: 3.2476, Train D Loss: 0.2346\n",
            "Epoch [187/200], Train G Loss: 3.2165, Train D Loss: 0.2561\n",
            "Epoch [188/200], Train G Loss: 3.1693, Train D Loss: 0.2313\n",
            "Epoch [189/200], Train G Loss: 3.1720, Train D Loss: 0.2359\n",
            "Epoch [190/200], Train G Loss: 3.1882, Train D Loss: 0.2351\n",
            "Epoch [191/200], Train G Loss: 3.1899, Train D Loss: 0.2309\n",
            "Epoch [192/200], Train G Loss: 3.2013, Train D Loss: 0.2407\n",
            "Epoch [193/200], Train G Loss: 3.2285, Train D Loss: 0.2328\n",
            "Epoch [194/200], Train G Loss: 3.2079, Train D Loss: 0.2378\n",
            "Epoch [195/200], Train G Loss: 3.2658, Train D Loss: 0.2206\n",
            "Epoch [196/200], Train G Loss: 3.2477, Train D Loss: 0.2265\n",
            "Epoch [197/200], Train G Loss: 3.2009, Train D Loss: 0.2382\n",
            "Epoch [198/200], Train G Loss: 3.2197, Train D Loss: 0.2270\n",
            "Epoch [199/200], Train G Loss: 3.2419, Train D Loss: 0.2427\n",
            "Epoch [200/200], Train G Loss: 3.1978, Train D Loss: 0.2217\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 200\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "hist = {\n",
        "    \"train_G_loss\":[],\n",
        "    \"train_D_loss\":[]\n",
        "}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    running_G_loss = 0.0\n",
        "    running_D_loss = 0.0\n",
        "\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        real_imgs = imgs.to(device)\n",
        "        real_labels = torch.ones(imgs.shape[0], 1).to(device)\n",
        "        fake_labels = torch.zeros(imgs.shape[0], 1).to(device)\n",
        "\n",
        "        #----Train Generator----\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Noise input for Generator\n",
        "        z = torch.randn((imgs.shape[0], latent_dim)).to(device)\n",
        "\n",
        "        gen_imgs = generator(z)\n",
        "        G_loss = criterion(discriminator(gen_imgs), real_labels)\n",
        "        running_G_loss += G_loss.item()\n",
        "\n",
        "        G_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        #----Train Discriminator----\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = criterion(discriminator(real_imgs), real_labels)\n",
        "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake_labels)\n",
        "        # detach for creating a copy, and do not update gradients of G\n",
        "        D_loss = (real_loss + fake_loss) / 2\n",
        "        running_D_loss += D_loss.item()\n",
        "\n",
        "        D_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "    epoch_G_loss = running_G_loss / len(dataloader)\n",
        "    epoch_D_loss = running_D_loss / len(dataloader)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Train G Loss: {epoch_G_loss:.4f}, Train D Loss: {epoch_D_loss:.4f}\")\n",
        "\n",
        "    hist[\"train_G_loss\"].append(epoch_G_loss)\n",
        "    hist[\"train_D_loss\"].append(epoch_D_loss)\n",
        "\n",
        "    if epoch % save_interval == 0:\n",
        "        save_image(gen_imgs.data[:25], f\"images/epoch_{epoch}.png\", nrow=5, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TKg1a1djjWM"
      },
      "source": [
        "# 4. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KPtOOuYtj2LI"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nUZyGnr9Tz-k"
      },
      "outputs": [],
      "source": [
        "input = torch.randn(3, latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dVIxoa4WjsZG"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    imgs = generator(input.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bc85KET2kOyA",
        "outputId": "5d676d21-df4e-4dde-c1c7-506966dc4f83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[-0.9998, -0.9983, -0.9999,  ..., -0.9998, -0.9999, -0.9994],\n",
              "          [-0.9997, -0.9990, -0.9982,  ..., -0.9991, -0.9990, -0.9994],\n",
              "          [-0.9998, -0.9999, -0.9998,  ..., -0.9990, -0.9981, -0.9997],\n",
              "          ...,\n",
              "          [-0.9998, -0.9947, -0.9994,  ..., -0.9999, -0.9994, -0.9997],\n",
              "          [-0.9999, -0.9998, -0.9984,  ..., -0.9989, -0.9987, -0.9991],\n",
              "          [-0.9999, -0.9991, -0.9990,  ..., -0.9983, -0.9993, -0.9981]]],\n",
              "\n",
              "\n",
              "        [[[-0.9983, -0.9996, -0.9996,  ..., -0.9988, -0.9996, -0.9984],\n",
              "          [-0.9999, -0.9994, -0.9990,  ..., -0.9990, -0.9998, -0.9977],\n",
              "          [-0.9998, -0.9995, -0.9952,  ..., -0.9992, -0.9996, -0.9998],\n",
              "          ...,\n",
              "          [-0.9991, -0.9984, -0.9999,  ..., -0.9990, -0.9988, -0.9992],\n",
              "          [-0.9998, -0.9993, -0.9997,  ..., -0.9979, -0.9986, -0.9996],\n",
              "          [-0.9991, -0.9984, -0.9978,  ..., -0.9999, -0.9996, -0.9998]]],\n",
              "\n",
              "\n",
              "        [[[-1.0000, -1.0000, -0.9998,  ..., -0.9999, -0.9999, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -0.9999, -1.0000],\n",
              "          [-0.9990, -0.9999, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
              "          ...,\n",
              "          [-1.0000, -1.0000, -0.9999,  ..., -1.0000, -1.0000, -0.9999],\n",
              "          [-0.9999, -0.9999, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
              "          [-0.9999, -1.0000, -1.0000,  ..., -1.0000, -0.9999, -0.9999]]]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IyPlPBfjkYxJ",
        "outputId": "d46c6b65-afc6-48d5-e6fc-4fadc74eb14a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image_tensor = ((imgs[0] + 1) / 2).cpu()\n",
        "\n",
        "\n",
        "plt.imshow(image_tensor.permute(1, 2, 0).detach().numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK7Xly42k19E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNH9g9xKpKRn69A227/IPRU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}